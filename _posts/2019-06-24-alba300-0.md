---
title: "뉴스 댓글 분석 프로젝트[0]: 프로젝트를 시작하며"
date: 2019-06-24
layout: post
categories:
  - python
  - dataanalysis
  - machinelearning
  - socialdata
  - comments analysis
published: true
image: https://www.dropbox.com/s/28cpxp5jvo2tjsc/0001.jpg?dl=1
---

## 온라인 뉴스 댓글은 정말 사람들의 목소리일까?

> "여기 오신 모든 분들 손을 들어주세요." (모두 손을 든다)
>
> "나는 인터넷 뉴스를 보지 않는다, 하시는 분 손 내려주세요." (조금 손을 내린다)
>
> "나는 인터넷 뉴스를 보고 댓글은 보지 않는다, 하시는 분 손 내려주세요." (좀 더 손을 내린다)
>
> "아직도 반이 넘는 많은 분들이 손을 들어주시고 계시네요. 이제 손 내려주셔도 됩니다. 감사합니다."
>
> -- 2018 PyConKR 발표를 시작하며

![0004](https://beomi-tech-blog.s3.amazonaws.com/img/2019-06-26-062433.jpg)

작년(2018) PyCon에서 "[온라인 뉴스 댓글은 정말 사람들의 목소리일까?](https://archive.pycon.kr/2018/program/51)" 라는 주제로 비공개 세션을 진행했다.

해당 발표의 CFP를 제출했을 때 사실 '딥러닝 일베 탐지기'와 '네이버 뉴스 댓글 분석' 이라는 두 가지 주제 중 한 가지를 선택해 진행할 것이라고 생각했지만, CFP에 붙었던 주제가 네이버 댓글 분석이어서 급하게 해당 프로젝트로 선회하게 되었다.

발표가 나고 2~3개월동안 해도 괜찮은걸까, 할 수 있을까 하는 수많은 고민을 하다 결국은 진행하기로 했다.

## 무엇을 했나? - 데이터 수집편

![](https://beomi-tech-blog.s3.amazonaws.com/img/2019-06-26-062446.jpg)

### 1) 데이터 수집하기

처음 분석시에는 네이버에 있는 뉴스를 가져와 모든 것을 분석해 보기로 했지만, 현실의 장벽에 부딪혔다. 총 기사의 개수가 억단위를 넘어가는 상황에서 해당 기사들을 모두 긁어오는 것은 현실적인 무리가 있다고 판단해 기간별 샘플링을 하는 방법을 통해 개수를 많이 줄여서 데이터 수집 부담을 줄였다.

![0017](https://beomi-tech-blog.s3.amazonaws.com/img/2019-06-26-062457.jpg)

### 2) 서버리스 크롤링

실제로 데이터를 가져오는 과정에서 AWS Lambda 서비스를 이용한 서버리스 크롤링을 통해 동시에 수백개+ 정도의 요청을 보내는 방식을 통해 보다 빠르게 데이터를 수집했다. 데이터를 저장하는 소스는 AWS S3를 이용해 Lambda에서 `boto3` 패키지를 이용해 보다 쉽게 업로드 할 수 있도록 구성했다.

![0027](https://beomi-tech-blog.s3.amazonaws.com/img/2019-06-26-062510.jpg)

그리고 실제로 람다 함수를 activate하는 boto3를 이용한 로컬 async 기반 코드를 실행해 아래와 같이 서버리스 크롤링이 오토스케일링으로 자동으로 다량으로 올라갈 수 있도록 환경을 구성해보았다.

![0033](https://beomi-tech-blog.s3.amazonaws.com/img/2019-06-26-062524.jpg)

### 3) 데이터 용량 줄이기

하지만 데이터를 모으는 것 만이 끝이 아니다. 원본 데이터는 `*.json` 파일로, 용량이 상대적으로 큰 상태였다. 따라서 이 용량을 줄이기 위해 `parquet` 파일 형식으로 변환해 저장하는 방식을 사용하면 용량과 파일 개수를 획기적으로 줄일 수 있다.

> 이때 파일 개수는 PySpark Dataframe의 partition 개수에 의존한다. 즉 Repartitioning을 통해 파티션 개수를 1개로 줄이면 parquet파일도 1개로 나오고, 100개로 설정하면 파일이 100개로 쪼개진다.
>
> 파일 개수가 적으면 Sequential I/O가 이루어지기 때문에 속도면에서 조금 유리하지만, 실제로 PySpark가 클러스터에 작업을 분배할 때 파티션 개수에 따라 Job distribution이 이뤄지기 때문에 이후 작업시 몇 개의 Worker Node를 이용할지에 대한 생각을 한 뒤 적정한 수로 나누는 것이 좋다.
>
> 또한 연도/월/일과 같이 기간별 탐색이 자주 있는 경우 파일 단계에서 필터링을 걸어줄 수 있기 때문에 기간별로 다른 파일을 만들어 저장하는 것도 좋은 방법이다.

실제로 파일을 변환한 경우 아래와 같이 용량의 87% 감량이 이뤄진 것을 볼 수 있다.

![0039](https://beomi-tech-blog.s3.amazonaws.com/img/2019-06-26-062533.jpg)

> Parquet 파일 자체가 유리하다기 보다는 압축되지 않는 json파일 vs Snappy 압축이 이뤄진 parquet파일 간 비교이기 때문에 압축의 유무가 좀 더 큰 영향을 미친다.
>
> 다만 json파일 각각을 gzip등을 이용해 저장할 경우 이후 수천~수만개의 파일을 읽을때 해당 파일을 list/get하는 비용만으로도 AWS S3 비용이 상당히 올라가게 된다. (S3는 파일 리스팅/파일 GET요청 횟수별로 비용이 매겨진다.)

### 4) 데이터 분석환경

위와 같이 `parquet` 파일로 정제한 뒤에는 데이터 분석을 하기가 좀 더 쉬워진다.

그리고 해당 파일을 읽어서 데이터 분석을 할 수 있는 환경은 크게 Pandas를 이용한 로컬 환경과 PySpark를 올린 AWS EMR환경을 이용했다.

![0042](https://beomi-tech-blog.s3.amazonaws.com/img/2019-06-26-062542.jpg)

## 이후 어떻게 진행되고 있나?

2018년 파이콘 발표를 끝낸 뒤, 데이터 전체를 찾는 방식에서 매일 데이터 분석 모델을 돌려보고 있다.

이에 따라 다음 글에서는 무슨 데이터를 어떻게 모으고 있는지, 그리고 어떻게 데이터를 적재하고 ETL작업을 하는지 등에 대해 알아본다.

